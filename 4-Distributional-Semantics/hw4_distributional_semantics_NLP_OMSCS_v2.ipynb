{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "RPRgDUI215D0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Distributional Semantics\n",
    "\n",
    "Distributional semantics models the \"meaning\" of words relative to other words that typically share the same context.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:38.303128Z",
     "start_time": "2025-03-24T20:06:38.164092Z"
    }
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "CsFulvnv2N4V",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "id": "W6WaMQ7AKP9s",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:38.931220Z",
     "start_time": "2025-03-24T20:06:38.164813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (2.16.1)\r\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (3.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (1.26.2)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (3.11.11)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (0.27.1)\r\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.12.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/envs/nlp_hw4/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "id": "VKofzznJxeat",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:41.016004Z",
     "start_time": "2025-03-24T20:06:38.932319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# REMOVE THIS CELL BEFORE SUBMITTING\n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:41.017792Z",
     "start_time": "2025-03-24T20:06:41.016119Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:41.027105Z",
     "start_time": "2025-03-24T20:06:41.019728Z"
    }
   },
   "outputs": [],
   "source": [
    "import hw4_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "a_STNMEi6dKN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TnwdoLF02UU-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will first work with a pre-specified set of word embeddings, called [GLOVE](https://nlp.stanford.edu/projects/glove/). We will download it and set up a few basic global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "id": "trlh1SHDxwDj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:52.901316Z",
     "start_time": "2025-03-24T20:06:41.028084Z"
    }
   },
   "outputs": [],
   "source": [
    "GLOVE_MODEL = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "GLOVE_VOCAB_SIZE = len(GLOVE_MODEL.key_to_index)\n",
    "GLOVE_EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uJ-O6njz2s0A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "pSy1RRPq2wol",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You must complete the code to compute analogies based on GLOVE embeddings.\n",
    "\n",
    "An analogy is of the form ``a:b :: c:d``.\n",
    "\n",
    "For example:\n",
    "\n",
    "``\n",
    "america : hamburger :: canada : ?\n",
    "``\n",
    "\n",
    "In this case we want to know what the `?` will be.\n",
    "\n",
    "To compute an analogy, first convert `a`, `b`, and `c` into vectors using GLOVE: ``glove[word]``.\n",
    "This will give you three vectors $\\overrightarrow{a}$, $\\overrightarrow{b}$, and $\\overrightarrow{c}$. Next compute $\\overrightarrow{d}=(\\overrightarrow{b}-\\overrightarrow{a})+\\overrightarrow{c}$.\n",
    "\n",
    "Unfortunately, $\\overrightarrow{d}$ might not correspond to any one word. Instead, find the `k` vectors that are most similar to $\\overrightarrow{d}$, and return the words that correspond to those vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "id": "EDlJ_Fec6FmN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:52.908882Z",
     "start_time": "2025-03-24T20:06:52.907064Z"
    }
   },
   "outputs": [],
   "source": [
    "# analogy is a:b :: c:d\n",
    "# america:canada :: hamburger:?\n",
    "# DO NOT USE most_similar()\n",
    "def glove_analogy(glove, a, b, c, k):\n",
    "  \"\"\"\n",
    "    Find the k most similar words to d_v (cosine similarity):\n",
    "    1. Normalize d_vector (by dividing it by its norm)\n",
    "    2. Get all word vectors in glove and normalize them\n",
    "    3. Multiply all other vectors with d_v to get the cosine similarity\n",
    "    4. Sort the results and return the top k similar vectors to d_v\n",
    "    5. Get the words corresponding to the top k similar vectors\n",
    "  \"\"\"\n",
    "  d_list = None\n",
    "  ### BEGIN SOLUTION\n",
    "  a_v, b_v, c_v = glove[a], glove[b], glove[c]\n",
    "  d_v = (b_v - a_v) + c_v\n",
    "  \n",
    "  # Normalize d_v\n",
    "  d_norm = d_v / np.linalg.norm(d_v)\n",
    "  \n",
    "  # Normalize all word vectors in glove\n",
    "  all_words = np.array(list(glove.key_to_index.keys()))\n",
    "  all_vectors = np.array([glove[w] for w in all_words])\n",
    "  all_vectors_n = np.linalg.norm(all_vectors, axis=1, keepdims=True)\n",
    "  all_norms = np.divide(all_vectors, all_vectors_n, where=all_vectors_n!=0)\n",
    "\n",
    "  # Get top k similar words to d_v\n",
    "  similarities = np.dot(all_norms, d_norm)\n",
    "  top_k = np.argpartition(-similarities, k)[:k]\n",
    "  sorted = np.argsort(-similarities[top_k])\n",
    "  sorted_top_k = top_k[sorted]\n",
    "\n",
    "  d_list = all_words[sorted_top_k].tolist()\n",
    "  ### END SOLUTION\n",
    "  return d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "id": "ljcPug22_psM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:53.496184Z",
     "start_time": "2025-03-24T20:06:53.002263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'pilot', 'jet', 'aircraft', 'plane', 'helicopter', 'air', 'planes', 'flight', 'flying']\n"
     ]
    }
   ],
   "source": [
    "d = glove_analogy(GLOVE_MODEL, 'driver', 'car', 'pilot', k=10)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "arXEXrtT1wze",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- **TODO:** grading. we can look to see if specific words are returned within the top k return results. Create a test list and a set of potential answers. If all (or any) are in the returned list then success. Depending on how variable the results can be. -->\n",
    "Test: Check if the glove_analogy function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "id": "mQE4-ri7p2sd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:06:55.502809Z",
     "start_time": "2025-03-24T20:06:53.523946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test A: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test A (5 points)\n",
    "ag.test_glove_analogy(GLOVE_MODEL, glove_analogy_fn=glove_analogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "9xlR9WxF9lG9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2J2Ft_wX9pbs",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this part of the assignment, we will use word vectors to perform document retrieval. Given a query term, retrieve the `k` most related documents.\n",
    "\n",
    "To do this, we will need to embed all the documents in a dataset into a document vector that can be compared to the query term vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "AUx54Kd67n9N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "IqazmKwB-Th8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The wikitext 2 dataset is a collection of high-quality documents from Wikipedia. We will load them into Panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "id": "CnXBk2DmyvvS",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.975906Z",
     "start_time": "2025-03-24T20:06:55.476429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b51d53523264c14b321ac7bd421dbb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_data_train = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"train\").shuffle()\n",
    "wiki_data_test = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"test\").shuffle()\n",
    "WIKI_TRAIN = pd.DataFrame(wiki_data_train)\n",
    "WIKI_TEST = pd.DataFrame(wiki_data_test)\n",
    "WIKI_ALL = pd.concat([WIKI_TRAIN, WIKI_TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "usosrBje-g2Z",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "PPIFKe0w-iSl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a default tokenizer that comes with  the `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "id": "5In_zJGE1kCa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.977669Z",
     "start_time": "2025-03-24T20:07:01.976108Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "76J8jcul2sn0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Optional:** If you wish to change or modify the tokenization of a string, you can add your own code to the following function.\n",
    "\n",
    "We will use `my_tokenizer` for tokenization tasks from this point forward. It will work even if you do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "id": "wwlhfq174B4s",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.983811Z",
     "start_time": "2025-03-24T20:07:01.978184Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(string):\n",
    "  tokens = TOKENIZER(string)\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "id": "1vJZdlc6_BOV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.983918Z",
     "start_time": "2025-03-24T20:07:01.980277Z"
    }
   },
   "outputs": [],
   "source": [
    "RETRIEVAL_MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ncE8kKVa_Q2N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Embed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2LRTIgeE_VMQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. The `embed_dataset()` function converts a Panda data frame into a numpy matrix of size `len(dataframe) x embedding_size`.\n",
    "\n",
    "Your code must iterate through all documents in `dataframe[text]`, tokenize each document, convert each token into a GLOVE vector, and take the average of embeddings in the same document as the embedding representation of the document.\n",
    "\n",
    "The numpy matrix is set up for you, so you must splice your vectors into the appropriate places in the matrix.\n",
    "\n",
    "**Hint:** create a numpy array for a document and use multi-dimensional numpy array slicing to insert it into the appropriate position in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "id": "WOwJf8xV16Ug",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.990874Z",
     "start_time": "2025-03-24T20:07:01.985810Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_dataset(dataframe, glove, tokenizer_fn=my_tokenizer, embed_size=GLOVE_EMBEDDING_SIZE, max_length=RETRIEVAL_MAX_LENGTH):\n",
    "  \"\"\"\n",
    "    Converts collection of documents into embeddings -- For each document in dataframe:\n",
    "    1. Process the document:\n",
    "       - Tokenize each document (produces a list of tokens - words)\n",
    "       - Convert tokens into a GLOVE word vectors (if present in GLOVE)\n",
    "       - Restrict the number of tokens to max_length for consistency\n",
    "    2. Place the vectors in the appropriate position in the embedded_data \n",
    "    3. Get the document embedding by averaging over word vectors per document in embedded_data (reduces to 2D)\n",
    "  \"\"\"\n",
    "  embedded_data = np.zeros((len(dataframe), max_length, embed_size))\n",
    "  ### BEGIN SOLUTION\n",
    "  \n",
    "  for i, doc in enumerate(dataframe[\"text\"]):\n",
    "    # Tokenize words in document and get its vectors\n",
    "    tokens = tokenizer_fn(doc)[:max_length]\n",
    "    vectors = np.array([glove[t] for t in tokens if t in glove])\n",
    "    \n",
    "    # Place vectors in the appropriate position before averaging\n",
    "    idx = vectors.shape[0]\n",
    "    if idx > 0:\n",
    "      embedded_data[i, :idx, :] = vectors \n",
    " \n",
    "  # Get document embedding per document\n",
    "  embedded_data = np.mean(embedded_data, axis=1)\n",
    "  \n",
    "  ### END SOLUTION\n",
    "  return embedded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "4pRdJ9tON4Mu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test. Hard code some words in a small custom dataframe and hard-code the glove embeddings, just need to do a simple accuracy check. -->\n",
    "Test: Check if the `embed_dataset` function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "id": "k7nZVisKdGCQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:01.993559Z",
     "start_time": "2025-03-24T20:07:01.987858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test B: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test B (10 points)\n",
    "ag.unit_test_embed_dataset(GLOVE_MODEL, embed_dataset_fn=embed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "id": "cDzZLG1z3T-S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:05.878999Z",
     "start_time": "2025-03-24T20:07:01.993318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36718, 100)\n"
     ]
    }
   ],
   "source": [
    "embedded_data = embed_dataset(WIKI_TRAIN, GLOVE_MODEL)\n",
    "print(embedded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aOAaFOi8CcW-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. `retrieve_top_k` takes a word and finds the top `k` documents in `embedded_data`, a matrix of size `num_docs x max_doc_length x embed_size`. Return the *indexes* of the top `k` most similar documents to the input word.\n",
    "\n",
    "**Hint:** you should not need to write a loop. You should be able to do everything through numpy matrix manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "id": "KpmFrJA_BlLZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:05.881730Z",
     "start_time": "2025-03-24T20:07:05.879672Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k(word, glove, embedded_data, k=10):\n",
    "  \"\"\"\n",
    "    If given word is in GLOVE, retrieve the top k most related documents to the word.\n",
    "    Calculate cosine similarity between word_vector and all document vectors\n",
    "      1. Normalize word_vector\n",
    "      2. Multiply all document vectors with word_vector to get the cosine similarity\n",
    "      4. Sort the results and return the top k similar document vectors to word_vector\n",
    "      5. Get the indexes of the top k similar document vectors\n",
    "  \"\"\"\n",
    "  top_k_docs = []\n",
    "  \n",
    "  ### BEGIN SOLUTION\n",
    "  if word in glove:\n",
    "    word_v = glove[word]\n",
    "    word_norm = word_v / np.linalg.norm(word_v)\n",
    " \n",
    "    similarities = np.dot(embedded_data, word_norm)\n",
    "    top_k_docs = np.argsort(similarities)[::-1][:k]\n",
    "  \n",
    "  ### END SOLUTION\n",
    "  return top_k_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "id": "VdYLH6cyDExx",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:05.978966Z",
     "start_time": "2025-03-24T20:07:05.892638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes: [35019  3257 13563  8239 28962 33718 35357 23834  9037  7737]\n"
     ]
    },
    {
     "data": {
      "text/plain": "35019     <unk> from Earth to other planets in the Sola...\n3257      On February 8 , 1992 , the Ulysses solar prob...\n13563     In 1981 , a proposal for an asteroid mission ...\n8239      There was a good deal of interest in the 2004...\n28962     The 2006 debate surrounding Pluto and what co...\n33718     Another major issue is the amount of radiatio...\n35357     Ceres is the largest object in the asteroid b...\n23834     Sometimes Venus only <unk> the Sun during a t...\n9037      The existence of an atmosphere on Venus was c...\n7737      Dawn 's mission profile calls for it to study...\nName: text, dtype: object"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'mars'\n",
    "# Retrieve indexes of top k most similar documents to the above word\n",
    "top_k = retrieve_top_k(word, GLOVE_MODEL, embedded_data, k=10)\n",
    "print(\"indexes:\", top_k)\n",
    "# Get the dataframe for the top k\n",
    "WIKI_TRAIN.iloc[top_k]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "id": "4yMDyA-7eRvh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:12.006916Z",
     "start_time": "2025-03-24T20:07:05.955873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test C: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test C (5 points)\n",
    "ag.unit_test_retrieve_top_k(GLOVE_MODEL, embed_dataset_fn=embed_dataset, retrieve_top_k_fn=retrieve_top_k, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uEHVza7_6iDC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2lSjyyyDDsH3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section, you will re-implement and train Word2Vec from scratch. There are two versions of Word2Vec. The first uses a continuous bag of words (CBOW) representation and the second uses skip grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "wE9wQg7xD9-b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "gc1KF8ogEDGr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following is a standard class that stores a vocabulary. The vocabulary object can:\n",
    "* Tell you all the words: `get_words()`\n",
    "* Tell you how many words there are: `num_words()`\n",
    "* Map a word to an index: `word2index()`\n",
    "* Map an index to a word: `index2word()`\n",
    "\n",
    "Additionally, it has two helper functions used during set up:\n",
    "* `add_word()` adds a word to the vocabulary.\n",
    "* `add_sentence()` adds all the previously unknown words in a sentence to the vocabulary (simply splitting the sentence by blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false,
    "id": "e7CBDQ5F21QV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:07:12.023306Z",
     "start_time": "2025-03-24T20:07:11.946088Z"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL BUT DO NOT EDIT IT\n",
    "UNK_token = 0   # Unknown '<unk>'\n",
    "UNK_symbol = '<unk>'\n",
    "\n",
    "class Vocab:\n",
    "  def __init__(self, name=''):\n",
    "    self.name = name\n",
    "    self._word2index = {UNK_symbol: UNK_token}\n",
    "    self._word2count = {UNK_symbol: 0}\n",
    "    self._index2word = {UNK_token: UNK_symbol}\n",
    "    self._n_words = 1\n",
    "\n",
    "  def get_words(self):\n",
    "    return list(self._word2count.keys())\n",
    "\n",
    "  def num_words(self):\n",
    "    return self._n_words\n",
    "\n",
    "  def word2index(self, word):\n",
    "    if word in self._word2index:\n",
    "      return self._word2index[word]\n",
    "    else:\n",
    "      return self._word2index[UNK_symbol]\n",
    "\n",
    "  def index2word(self, word):\n",
    "    return self._index2word[word]\n",
    "\n",
    "  def word2count(self, word):\n",
    "    return self._word2count[word]\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self._word2index:\n",
    "      self._word2index[word] = self._n_words\n",
    "      self._word2count[word] = 1\n",
    "      self._index2word[self._n_words] = word\n",
    "      self._n_words += 1\n",
    "    else:\n",
    "      self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "JnFgIfwk_bwv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0fn9kKpcFmnI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The continuous bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kXMUVEn-zbw_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true,
    "id": "GNd7tZotL_EE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:41.714009Z",
     "start_time": "2025-03-24T20:14:41.696045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change them\n",
    "CBOW_EMBED_DIMENSIONS = 100\n",
    "CBOW_WINDOW = 10\n",
    "CBOW_MAX_LENGTH = 50\n",
    "CBOW_BATCH_SIZE = 1024\n",
    "CBOW_NUM_EPOCHS = 2\n",
    "CBOW_LEARNING_RATE = 5e-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "r-o1tDEfFp5d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the CBOW model, we must prepare the data for training. The CBOW model learns to predict a word based on the words to the left and the words to the right.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `y` is the index of a word in the corpus.\n",
    "* `x` is a list of indexes of words to the left of `y` and to the right of `y`.\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=[the, quick, fox, jumped]`, `y=brown`\n",
    "2. `x=[quick, brown, jumped, over]`, `y=fox`\n",
    "3. `x=[brown, fox, over, the]`, `y=jumped`\n",
    "4. `x=[fox, jumped, the, lazy]`, `y=over`\n",
    "5. `x=[jumped, over, lazy, dog]`, `y=the`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabulary)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_cbow_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_cbow_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TOZwYPnLzf4C",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the `prep_cbow_data()` function. It takes a data frame and a tokenizer (`my_tokenizer()`) a window to either side of each word, and a max document length. The function should return two values as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true,
    "id": "zIERa_oHDjiq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:45.208484Z",
     "start_time": "2025-03-24T20:14:45.188766Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_cbow_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "  \"\"\"\n",
    "    1. Tokenize each document, restricting the # of tokens to max_length.\n",
    "    2. Add ALL tokens to the vocabulary.\n",
    "    3. Iterate over each token, skipping the window size at the beginning and end, setting the target word to the current token, and the context words to the words within the window.\n",
    "    5. Convert selected words to their index in the vocabulary.\n",
    "    6. Append the (context, target) pairs to the dataset.\n",
    "  \"\"\"\n",
    "  data_out = []\n",
    "  vocab = Vocab()\n",
    "  ### BEGIN SOLUTION\n",
    "  \n",
    "  for doc in data_frame['text']:\n",
    "    tokens = tokenizer_fn(doc)[:max_length]\n",
    "    unique_tokens = list(set(tokenizer_fn(doc)))\n",
    "    vocab.add_sentence(' '.join(unique_tokens))\n",
    "      \n",
    "    for i in range(window, len(tokens)-window):\n",
    "      # x = context words list, y = target word\n",
    "      target = tokens[i]\n",
    "      context = tokens[i-window : i] + tokens[i+1 : i+window+1]\n",
    "      \n",
    "      y = vocab.word2index(target)\n",
    "      x = [vocab.word2index(w) for w in context]\n",
    "      data_out.append((x,y))\n",
    "          \n",
    "  ### END SOLUTION\n",
    "  return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": false,
    "id": "2W6tb7s7GMjG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:49.462708Z",
     "start_time": "2025-03-24T20:14:46.000084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataframe= 36718 len data= 433436\n"
     ]
    }
   ],
   "source": [
    "CBOW_DATA, CBOW_VOCAB = prep_cbow_data(WIKI_TRAIN, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n",
    "print(\"len dataframe=\", len(WIKI_TRAIN), \"len data=\", len(CBOW_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "51d1xzfxQpFb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " <!-- Unit test: Do something along the lines of figuring out how many words are in lines with greater than window*2+1 words. What I have below isn't quite matching what my solution above is producing. I'm not sure if my solution above has a bug or if my computation below is incorrect, or if it is just an approximation and we should allow some variance. -->\n",
    " Test: checking the size of the dataset and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": false,
    "id": "mBHltjJgRgqD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:50.713041Z",
     "start_time": "2025-03-24T20:14:49.462956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected data points 433436\n",
      "actual data points 433436\n",
      "difference 0\n",
      "least vocab size 28782\n",
      "actual vocab size 28783\n",
      "\n",
      "Test passed!\n",
      "Test D: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test D (10 points)\n",
    "ag.check_data_size_d(WIKI_TRAIN, CBOW_WINDOW, CBOW_DATA, CBOW_VOCAB, max_length=CBOW_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GcabfK1P0EU3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Get Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqnjtfprK2jb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the following function. `get_batch()` will return a batch of data of the given size, starting at the given index.\n",
    "\n",
    "The function should return two values:\n",
    "1. A batch of `x` components of the data as a tensor of size `window*2 x batch_size`.\n",
    "2. A batch of `y` components of the data as a tensor array of length `window*2`.\n",
    "\n",
    "Both tensors should be moved to the GPU, if available, before being returned (Note: Gradescope will not have a GPU available).\n",
    "\n",
    "**Hint:** You should not need to write a loop. You can achieve what you need using numpy slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true,
    "id": "OhL7ncHyBX9y",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:50.715318Z",
     "start_time": "2025-03-24T20:14:50.713558Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(data, index, batch_size=10):\n",
    "  ### BEGIN SOLUTION\n",
    "  batch_data = data[index : index+batch_size]\n",
    "  x = np.array([d[0] for d in batch_data], dtype=np.int64)\n",
    "  y = np.array([d[1] for d in batch_data], dtype=np.int64)\n",
    "\n",
    "  x = torch.tensor(x, dtype=torch.long).to(DEVICE)\n",
    "  y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n",
    "  ### END SOLUTION\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uwiTTNSlOpGK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test: make up some synthetic data, check if you get the right stuff out for a given idx and batch size. -->\n",
    "Test: Check if get back works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": false,
    "id": "CrTiQ-WOULcZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:55.054748Z",
     "start_time": "2025-03-24T20:14:55.033170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test E: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test E (10 points)\n",
    "ag.unit_test_get_batch(CBOW_DATA, CBOW_WINDOW, 10, get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "_jRn11uf0IrR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aq09TkU6UtMn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the CBOW model specification.\n",
    "\n",
    "The CBOW model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a list of `window*2` indices and produce a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true,
    "id": "k2eQkT122Am3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:57.497823Z",
     "start_time": "2025-03-24T20:14:57.486390Z"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(CBOW, self).__init__()\n",
    "    ### BEGIN SOLUTION\n",
    "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x):\n",
    "    probs = None\n",
    "    ### BEGIN SOLUTION\n",
    "    x = self.embed(x).mean(dim=1) ## Get document embedding\n",
    "    out = self.linear(x)\n",
    "    probs = F.log_softmax(out, dim=1)\n",
    "    ### END SOLUTION\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UZLSvKvWVmwT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": false,
    "id": "6vMBdIU8fz0h",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:58.327409Z",
     "start_time": "2025-03-24T20:14:58.281780Z"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n",
    "cbow_model.to(DEVICE)\n",
    "CBOW_CRITERION = nn.NLLLoss()\n",
    "try:\n",
    "  CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "amzCDmGi6XeD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Check the structure of CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": false,
    "id": "_RVJeGf1oJla",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:14:59.102443Z",
     "start_time": "2025-03-24T20:14:59.089573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has two layers as expected!\n",
      "Your layers orders are as expected!\n",
      "Test F: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - Test F (10 points)\n",
    "ag.test_cbow_structure(cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "WcH2bQU50hJM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kvk4DBN8Vpk5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": false,
    "id": "w5kYEX41LKKj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:15:01.680537Z",
     "start_time": "2025-03-24T20:15:01.668882Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cbow(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item())\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ynoHvwSTVsv6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": false,
    "id": "CTpPWtKtB07T",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:16:20.033851Z",
     "start_time": "2025-03-24T20:15:02.636708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 10.279401779174805\n",
      "iter 100 loss 9.491836207928044\n",
      "iter 200 loss 8.33357855811048\n",
      "iter 300 loss 7.287590009429526\n",
      "iter 400 loss 6.570369159194299\n",
      "epoch 0 loss 6.442605366943575\n",
      "iter 0 loss 4.2993268966674805\n",
      "iter 100 loss 3.7909741212825963\n",
      "iter 200 loss 3.4759055690385807\n",
      "iter 300 loss 3.2582846875998666\n",
      "iter 400 loss 3.11861302430493\n",
      "epoch 1 loss 3.0965112901466676\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "FYtxtDr1Yf7U",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Now that we have trained the CBOW model, we will be testing it on the `WIKI_TEST` dataset. Your CBOW model will need to achieve an accuracy of at least 30% to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": false,
    "id": "Y0nbJltGqR8B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:16:20.222110Z",
     "start_time": "2025-03-24T20:16:20.038631Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "      token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "      for i in range(window, len(token_ids)-window):\n",
    "        x = token_ids[i-window:i] + token_ids[i+1:i+window+1]\n",
    "        y = token_ids[i]\n",
    "        data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": false,
    "id": "tnVcs4HQtXV_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:16:22.571280Z",
     "start_time": "2025-03-24T20:16:20.164729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Accuracy = 0.3186107575893402/20\n",
      "Test G: 20/20\n"
     ]
    }
   ],
   "source": [
    "# student check - G (20 points)\n",
    "ag.test_cbow_performance(cbow_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "d7vzldCV_Xhm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Skip Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qIcLx44H00tj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The Skip Gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true,
    "id": "UDGcOm6L_WxO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:38.218842Z",
     "start_time": "2025-03-24T20:08:38.211740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change\n",
    "SKIP_EMBED_DIMENSIONS = 100\n",
    "SKIP_WINDOW = 8\n",
    "SKIP_MAX_LENGTH = 50\n",
    "SKIP_BATCH_SIZE = 1024\n",
    "SKIP_NUM_EPOCHS = 2\n",
    "SKIP_LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "tc1pGyIN02q0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the Skip Gram model, we must prepare the data for training. The Skip Gram model learns to predict words to the left and right of a given word.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `x` is the index of a word in the corpus.\n",
    "* `y` is a list of indexes of words to the left of `x` or to the right of `x`.\n",
    "(Note the organization of the data is the opposite of the CBOW model)\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=brown`, `y=[the, quick, fox, jumped]`\n",
    "2. `x=fox`, `y=[quick, brown, jumped, over]`\n",
    "3. `x=jumped`, `y=[brown, fox, over, the]`\n",
    "4. `x=over`, `y=[fox, jumped, the, lazy]`\n",
    "5. `x=the`, `y=[jumped, over, lazy, dog]`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabular)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_skip_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_skip_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data, where each `y` is a list of word indices\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true,
    "id": "pKPaXDir-8ef",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:38.218941Z",
     "start_time": "2025-03-24T20:08:38.215960Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "  \"\"\"\n",
    "    1. Tokenize each document, restricting the # of tokens to max_length.\n",
    "    2. Add ALL tokens to the vocabulary.\n",
    "    3. Iterate over each token, skipping the window size at the beginning and end, setting the target word to the current token, and the context words to the words within the window.\n",
    "    5. Convert selected words to their index in the vocabulary.\n",
    "    6. Append the (context, target) pairs to the dataset.\n",
    "  \"\"\"\n",
    "  data_out = []\n",
    "  vocab = Vocab()\n",
    "  \n",
    "  ### BEGIN SOLUTION\n",
    "  for doc in data_frame['text']:\n",
    "    tokens = tokenizer_fn(doc)[:max_length]\n",
    "    unique_tokens = list(set(tokenizer_fn(doc)))\n",
    "    vocab.add_sentence(' '.join(unique_tokens))\n",
    "      \n",
    "    for i in range(window, len(tokens)-window):\n",
    "      # x = target word, y = context words list\n",
    "      target = tokens[i]\n",
    "      context = tokens[i-window : i] + tokens[i+1 : i+window+1]\n",
    "      \n",
    "      x = vocab.word2index(target)\n",
    "      y = [vocab.word2index(w) for w in context]\n",
    "      data_out.append((x,y))\n",
    "  ### END SOLUTION\n",
    "  \n",
    "  return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": false,
    "id": "HqoX7hkU_gf6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:41.244320Z",
     "start_time": "2025-03-24T20:08:38.231300Z"
    }
   },
   "outputs": [],
   "source": [
    "SKIP_DATA, SKIP_VOCAB = prep_skip_gram_data(WIKI_TRAIN, my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": false,
    "id": "tsWgOrt-QVoB",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:41.246626Z",
     "start_time": "2025-03-24T20:08:41.244907Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_DATA[0]\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8a8y6vLKDe1f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: compute the number of data points that should be in SKIP_DATA and check the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": false,
    "id": "YpNy1Qg1sK1u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.435946Z",
     "start_time": "2025-03-24T20:08:41.254042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected data points 495049\n",
      "actual data points 495049\n",
      "difference 0\n",
      "least vocab size 28782\n",
      "actual vocab size 28783\n",
      "\n",
      "Test passed!\n",
      "Test H: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - H (5 points)\n",
    "ag.check_data_size_h(WIKI_TRAIN, SKIP_WINDOW, SKIP_DATA, SKIP_VOCAB, max_length=SKIP_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "jYzbwH0BDui4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Skip Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "u03pc5UiD2db",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the Skip Gram model specification.\n",
    "\n",
    "The Skip Gram model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a single token index and produces a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true,
    "id": "QmBQWNxS5zAd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.443435Z",
     "start_time": "2025-03-24T20:08:42.436797Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(SkipGram, self).__init__()\n",
    "    ### BEGIN SOLUTION\n",
    "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x):\n",
    "    probs = None\n",
    "    ### BEGIN SOLUTION\n",
    "    x = self.embed(x)\n",
    "    out = self.linear(x)\n",
    "    probs = F.log_softmax(out, dim=1)\n",
    "    ### END SOLUTION\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqgDR_KbEGaf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: check the layers and layer ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.482342Z",
     "start_time": "2025-03-24T20:08:42.438898Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "skip_model = SkipGram(SKIP_VOCAB.num_words(), SKIP_EMBED_DIMENSIONS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": false,
    "id": "s_Td11UJQIcv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.485878Z",
     "start_time": "2025-03-24T20:08:42.482627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has two layers as expected!\n",
      "Your layers orders are as expected!\n",
      "Test passed!\n",
      "Test I: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - Test I (5 points)\n",
    "ag.test_skipgram_structure(skip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UoCtW4HhDniu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": false,
    "id": "dfeBc-qu_MkK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.487833Z",
     "start_time": "2025-03-24T20:08:42.485836Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_CRITERION = nn.NLLLoss()\n",
    "  SKIP_OPTIMIZER = torch.optim.AdamW(skip_model.parameters(), lr=SKIP_LEARNING_RATE)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": false,
    "id": "tpyRM2LjypCq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:08:42.501509Z",
     "start_time": "2025-03-24T20:08:42.489032Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_skipgram(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = None\n",
    "      # Calculate loss for every word in the context\n",
    "      for word in y.T:\n",
    "        if loss is None:\n",
    "          loss = criterion(y_hat, word)\n",
    "        else:\n",
    "          loss += criterion(y_hat, word)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item() / y.shape[1])\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": false,
    "id": "RZOfUwgsEUQe",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:11:52.573421Z",
     "start_time": "2025-03-24T20:08:42.491945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 10.41059684753418\n",
      "iter 100 loss 9.564102475005802\n",
      "iter 200 loss 8.599997643807635\n",
      "iter 300 loss 7.786588515158112\n",
      "iter 400 loss 7.180968622317041\n",
      "epoch 0 loss 6.816049751534472\n",
      "iter 0 loss 5.433618068695068\n",
      "iter 100 loss 4.840738735576667\n",
      "iter 200 loss 4.586633184062901\n",
      "iter 300 loss 4.4613200026097095\n",
      "iter 400 loss 4.389866654117803\n",
      "epoch 1 loss 4.361723249249824\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  train_skipgram(skip_model, SKIP_DATA, num_epochs=SKIP_NUM_EPOCHS, batch_size=SKIP_BATCH_SIZE, criterion=SKIP_CRITERION, optimizer=SKIP_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VdEsOoV4PlTh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we have trained the Skipgram model, we will be using the `WIKI_TEST` dataset again for evaluation. Your Skipgram model will need to achieve at least 30% accuracy to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": false,
    "id": "tmNDlKPRNR1G",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:11:52.962546Z",
     "start_time": "2025-03-24T20:11:52.885846Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "        token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "    for i in range(window, len(token_ids)-window):\n",
    "      x = token_ids[i]\n",
    "      y = token_ids[i-window:i]\n",
    "      y.extend(token_ids[i+1:i+1+window])\n",
    "      data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_skip_gram_test_data(WIKI_TEST, SKIP_VOCAB, tokenizer_fn=my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": false,
    "id": "fmVj4r1EtEr7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T20:11:55.531972Z",
     "start_time": "2025-03-24T20:11:52.963833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Accuracy = 0.3788052262931034/1\n",
      "Test J: 20/20\n"
     ]
    }
   ],
   "source": [
    "# student check - Test J (20 points)\n",
    "ag.test_skip_performance(skip_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "Please submit this .ipynb file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T22:50:47.471464Z",
     "start_time": "2025-03-24T22:50:47.444624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your projected points for this assignment is 130/100.\n",
      "\n",
      "NOTE: THIS IS NOT YOUR FINAL GRADE. YOUR FINAL GRADE FOR THIS ASSIGNMENT WILL BE AT LEAST 130 OR MORE, BUT NOT LESS\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-24T22:50:50.901353Z",
     "start_time": "2025-03-24T22:50:50.843481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time in minutes = 164.21134543418884\n",
      "WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\n"
     ]
    }
   ],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
