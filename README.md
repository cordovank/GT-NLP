# NLP — Foundations → Applied QA
>Coursework & projects progressing from linear baselines to sequence models, attention, distributional semantics, and memory-augmented QA.

[![Python](https://img.shields.io/badge/Python-3.10-informational)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)]()
![Tasks](https://img.shields.io/badge/Tasks-Classification%20%7C%20LM%20%7C%20Embeddings%20%7C%20QA-blue)
[![License: All Rights Reserved](https://img.shields.io/badge/License-All%20Rights%20Reserved-lightgrey.svg)](#license)
[![No AI Training](https://img.shields.io/badge/AI%20Training-Not%20Permitted-red.svg)](#license)

Hands-on progression from **classical NLP baselines** to **sequence models + attention** and **memory-augmented QA**.

## Projects
- **[Neural Networks with PyTorch](hw1_pytorch_basics/)**  
  MLP warm-up: tensors, batching, training loop, eval.
- **[Text Classification (IMDb & AG News)](hw2_text_classification/)**  
  BOW + Naive Bayes/LogReg vs embedding-based classifiers; accuracy/F1.
- **[Language Modeling (RNN)](hw3a_rnn_lm/)**  
  Next-token LM; perplexity; greedy & temperature sampling.
- **[LSTM + Attention](hw3b_lstm_attention/)**  
  LSTM LM with seq2seq-style attention; lower PPL, better coherence.
- **[Distributional Semantics](hw4_distributional_semantics/)**  
  GloVe analogies/retrieval; CBOW & Skip-Gram training from scratch.
- **[Key-Value Memory Networks (QA)](hw5_kv_memory_qa/)**  
  Single-hop QA over structured facts with batched attention.


## License
**All Rights Reserved.** No use, copying, modification, distribution, or model training is permitted without prior written permission.  
See the [LICENSE](./LICENSE) file for details.
